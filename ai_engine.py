import pickle
import pandas as pd
import os
import google.generativeai as genai
import pyarabic.araby as araby
from nltk.stem.isri import ISRIStemmer
import zipfile
import streamlit as st
import re  
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC

@st.cache_resource
def load_nafsbot_models():
    # ๐ ููุชุงุญ API
    os.environ["GOOGLE_API_KEY"] = "AIzaSyCK1kMchDgsxFPDHU3t2hXhn-h6sDOnHho"
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
    model = genai.GenerativeModel('gemini-3-flash')
    
    stemmer = ISRIStemmer()

    # --- ๐ฅ ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุชูุฏูุฉ (Normalization) ---
    def normalize_arabic_word(word):
        word = araby.strip_tatweel(word)      # ุฅุฒุงูุฉ ุงูุชุทููู (ู)
        word = araby.strip_tashkeel(word)     # ุฅุฒุงูุฉ ุงูุชุดููู
        word = re.sub(r'[ุฅุฃุขุง]', 'ุง', word)   # ุชูุญูุฏ ุงูุฃูู
        word = re.sub(r'ู', 'ู', word)        # ุชูุญูุฏ ุงููุงุก
        word = re.sub(r'ุค', 'ุก', word)        # ุชูุญูุฏ ุงูููุฒุงุช
        word = re.sub(r'ุฆ', 'ุก', word)
        word = re.sub(r'ุฉ', 'ู', word)        # ุงูุชุงุก ุงููุฑุจูุทุฉ -> ูุงุก
        word = re.sub(r'(.)\1{2,}', r'\1', word) # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑ (ูุซู: ุงุงุงููุง -> ุงููุง)
        return word
    
    # ุฏุงูุฉ ุงูุชูุทูุน (ุชุณุชุฎุฏู ุงููุนุงูุฌุฉ ุฃููุงู)
    def stem_arabic_word(text):
        try:
            text = normalize_arabic_word(text) # ุฃููุงู: ูุนุงูุฌุฉ ุงููุต
            words = text.split()
            # ุซุงููุงู: ุงุณุชุฎุฑุงุฌ ุงูุฌุฐุฑ
            return " ".join([stemmer.stem(word) for word in words])
        except: return text
    
    try:
        svm_model, df_data = None, None
        
        # ุชุญููู SVM (ุญุณุจ ุงุณู ุงูููู ุนูุฏู: svm_model.zip)
        if os.path.exists('svm_model.zip'):
            with zipfile.ZipFile('svm_model.zip', 'r') as z:
                pkl_files = [n for n in z.namelist() if n.endswith('.pkl')]
                if pkl_files:
                    with z.open(pkl_files[0]) as f: svm_model = pickle.load(f)
        elif os.path.exists('svm_model.pkl'):
            with open('svm_model.pkl', 'rb') as f: svm_model = pickle.load(f)

        # ุชุญููู Dataset (ุญุณุจ ุงุณู ุงูููู ุนูุฏู: dataset_original.zip)
        if os.path.exists('dataset_original.zip'):
            with zipfile.ZipFile('dataset_original.zip', 'r') as z:
                pkl_files = [n for n in z.namelist() if n.endswith('.pkl')]
                if pkl_files:
                    with z.open(pkl_files[0]) as f: df_data = pd.read_pickle(f)
        elif os.path.exists('dataset_original.pkl'):
            df_data = pd.read_pickle('dataset_original.pkl')

        # ุชุญููู ุงููููุงุช ุงูุตุบูุฑุฉ
        if os.path.exists('vectorizer.pkl'):
            with open('vectorizer.pkl', 'rb') as f: vec = pickle.load(f)
        if os.path.exists('label_encoder.pkl'):
            with open('label_encoder.pkl', 'rb') as f: enc = pickle.load(f)
        
        if svm_model is None or df_data is None:
            return None

        return {'model': model, 'svm': svm_model, 'vectorizer': vec, 
                'encoder': enc, 'data': df_data, 'stem': stem_arabic_word}
    except Exception as e:
        return None

def get_nafsbot_response(models, patient_input,chat_history):
    try:
        processed = models['stem'](patient_input)
        vec = models['vectorizer'].transform([processed]).toarray()
        pred_idx = models['svm'].predict(vec)[0]
        category = models['encoder'].inverse_transform([pred_idx])[0]
        
        related = models['data'][models['data']['Hierarchical Diagnosis'] == category]
        if len(related) == 0:
            # ุฅุฐุง ูู ูุฌุฏ ุฃู ุจูุงูุงุช ุทุจูุฉุ ูุฑูุถ ุงูุฅุฌุงุจุฉ ููุฑุงู ููุง ูุณุฃู Gemini
            return category, "ูุง ุนูุฏู ูุนูููุฉ ุทุจูุฉ ุฏูููุฉ ุนู ุญุงูุชู ูู ูุงุนุฏุฉ ุจูุงูุงุชู ุญุงููุงู. ุจูุตุญู ุชุณุชุดูุฑ ูุฎุชุต ุนุดุงู ุชููู ูุชุทูู ุฃูุซุฑ"
   
        context_str = ""
        if len(related) > 0:
            context = related.sample(n=min(3, len(related)))[['Question', 'Answer']].to_dict('records')
            for item in context: context_str += f"- {item['Question']}\n"
        
        prompt = f"""
    ุชุตุฑู ูู "ููุณ ุจูุช"ุ ุตุฏูู ููุฑุจ ูุฏุงุนู ููุณู ุญููู.
    ุงููุณุชุฎุฏู ุจููุฑ ุจุญุงูุฉ ุชู ุชุตููููุง ูู: {category}

    ุฅููู ุจุนุถ ุงูุญุงูุงุช ุงูุณุงุจูุฉ ููุฑุฌุน (ุฎุฐ ูููุง ุงููุงุฆุฏุฉ ุจุณ ูุง ุชูุณุฎูุง):
    {context_str}

    ุงููุณุชุฎุฏู ุจูุญูููู: "{patient_input}"

    ุงููุทููุจ ููู:
    1. ุฑุฏ ุนููู ุจููุฌุฉ ุนุงููุฉ ุจูุถุงุก (ูุฑูุจุฉ ูููู) ูุจุฃุณููุจ "ุตุฏูู ูุตุฏููู".
    2. ููู ูุชุนุงุทู ุฌุฏุงูุ ุทูููุ ูุญุณุณู ุฅูู ุฌูุจู ูุณุงูุนู.
    3. ุฃุนุทูู ูุตูุญุฉ ุจุณูุทุฉ ูุนูููุฉ ุจูุงุกู ุนูู ุงูุณูุงู ุงูุทุจู ุจุณ ุจูููุงุช ุจุณูุทุฉ ูุด ูุนูุฏุฉ.
    4. ุฎูู ุงูุฑุฏ ูุตูุฑ ููุจุงุดุฑ (ูู 3 ูู 4 ุฌูู).
    5.  ุงุฐุง ูุงู ููุงู ุงู ููุน ูู ุงููุงุน ููุฉ ุงูููุช ุงู ุงูุฐุงุก ุงูููุณ ุงู ุงูุงูุชุญุงุฑ ุงุนุทู ุงุฌุงุจุงุช ุชุฏุนู ููุบุงูุฉ ูููุฑ ุฑูู ุงูุทูุงุฑุฆ ููุฏุนู ุงูููุณู 0795785095 ุงู ุงูุทูุงุฆ ุงูุนุงูุฉ911 ูู ุงูุงุฑุฏู
    ุฃูุช "ููุณ ุจูุช"ุ ุตุฏูู ุฐูู ููุณุงุนุฏ ููุฏุนู ุงูููุณู ููุท.
    ุณุฌู ุงููุญุงุฏุซุฉ ุงูุณุงุจูุฉ:
    {chat_history}
    ุชุนูููุงุช ุตุงุฑูุฉ ููููุฉ ุฌุฏุงู:
    1. ุงูุฑุฃ ุฑุณุงูุฉ ุงููุณุชุฎุฏู ุฌูุฏุงู: "{patient_input}"
    2. ุญุฏุฏ ุงูููุถูุน:
       - ุฅุฐุง ูุงู ุงูููุงู ุนู ูุดุงุนุฑุ ุถููุ ุฎููุ ุงูุชุฆุงุจุ ูุถูุถุฉุ ุฃู ุชุญูุฉ (ูุฑุญุจุงุ ูููู): ููู ูุฌุงูุจ ูุตุฏูู ุฏุงุนู.
       - ุฅุฐุง ูุงู ุงูููุงู ุนู (ุทุจุฎุ ุฑูุงุถุฉุ ุณูุงุณุฉุ ุญู ูุงุฌุจุงุชุ ูุนูููุงุช ุนุงูุฉุ ุจูุน ูุดุฑุงุก): **ุชููู ููุฑุงู**.

    3. ูู ุญุงูุฉ ุงูุณุคุงู ุงูุฎุงุฑุฌู (ุบูุฑ ููุณู):
       - ุงุนุชุฐุฑ ุจูุทู ุดุฏูุฏ ูุจุงูุนุงููุฉ.
       - ูู ูู ุฌููุฉ ุจูุนูู: "ุณุงูุญูู ูุง ุบุงููุ ุฃูุง ููู ุจุณ ุนุดุงู ุฃุณูุนู ูุฃุฏุนูู ููุณูุงูุ ูุง ุนูุฏู ุฎุจุฑุฉ ุจููู ููุงุถูุน".
       - ูุง ุชุฌุจ ุนูู ุงูุณุคุงู ุฃุจุฏุงู.

    4. ูู ุญุงูุฉ ุงูููุงู ุงูููุณู ุฃู ุงููุถูุถุฉ:
       - ุชุตููู ุงูุญุงูุฉ: {category}
       - ุงูุณูุงู ุงูุทุจู ูููุณุงุนุฏุฉ: {context_str}
       - ุฑุฏ ุนููู ุจููุฌุฉ ุนุงููุฉ ุจูุถุงุกุ ุจุฃุณููุจ ุตุฏูู ููุฑุจ ูุญูููุ ูุทููู.
    """
        response = models['model'].generate_content(prompt)
        return category, response.text
    except Exception as e:
        return "Unknown", "ุนุฐุฑุงูุ ุญุฏุซ ุฎุทุฃ ูู ุงูุงุชุตุงู."
    
def retrain_model(original_data_path, new_data_df):
    """ุฏูุฌ ุงูุจูุงูุงุช ูุฅุนุงุฏุฉ ุชุฏุฑูุจ ุงูููุฏูู"""
    try:
        # 1. ุชุญููู ุงูุจูุงูุงุช ุงูุฃุตููุฉ
        if os.path.exists(original_data_path):
            if original_data_path.endswith('.zip'):
                with zipfile.ZipFile(original_data_path, 'r') as z:
                    with z.open([n for n in z.namelist() if n.endswith('.pkl')][0]) as f:
                        df_old = pd.read_pickle(f)
            else:
                df_old = pd.read_pickle(original_data_path)
        else:
            return False, "ููู ุงูุจูุงูุงุช ุงูุฃุตูู ุบูุฑ ููุฌูุฏ"

        # 2. ุงูุฏูุฌ (Merge)
        # ุงูุชุฃูุฏ ูู ุชุทุงุจู ุงูุฃุนูุฏุฉ
        new_data_df = new_data_df[['Question', 'Answer', 'Hierarchical Diagnosis']]
        df_combined = pd.concat([df_old, new_data_df]).drop_duplicates(subset=['Question']).reset_index(drop=True)

        # 3. ุฅุนุงุฏุฉ ุงูุชุฏุฑูุจ (Retraining)
        # ุงููุนุงูุฌุฉ
        stemmer = ISRIStemmer()
        df_combined['processed'] = df_combined['Question'].apply(lambda x: advanced_arabic_processing(str(x)))
        
        # Vectorizer
        cv = CountVectorizer()
        X = cv.fit_transform(df_combined['processed']).toarray()
        
        # Encoder
        le = LabelEncoder()
        y = le.fit_transform(df_combined['Hierarchical Diagnosis'])
        
        # SVM Training
        clf = SVC(kernel='linear')
        clf.fit(X, y)

        # 4. ุญูุธ ุงููููุงุช ุงูุฌุฏูุฏุฉ
        # ุญูุธ ุงูููุฏููุงุช ุงูุตุบูุฑุฉ
        with open('svm_model.pkl', 'wb') as f: pickle.dump(clf, f)
        with open('vectorizer.pkl', 'wb') as f: pickle.dump(cv, f)
        with open('label_encoder.pkl', 'wb') as f: pickle.dump(le, f)
        # ุญูุธ ุงูุฏุงุชุง ุณูุช ุงูุฌุฏูุฏุฉ
        df_combined.to_pickle('dataset_original.pkl')

        # ุถุบุท ุงููููุงุช ุงููุจูุฑุฉ (ูู GitHub)
        with zipfile.ZipFile('svm_model.zip', 'w', zipfile.ZIP_DEFLATED) as z:
            z.write('svm_model.pkl')
        with zipfile.ZipFile('dataset_original.zip', 'w', zipfile.ZIP_DEFLATED) as z:
            z.write('dataset_original.pkl')
            
        return True, f"ุชู ุงูุชุฏุฑูุจ ุจูุฌุงุญ! ุนุฏุฏ ุงูุจูุงูุงุช ุงูุฌุฏูุฏ: {len(df_combined)}"
    except Exception as e:
        return False, f"ูุดู ุงูุชุฏุฑูุจ: {str(e)}"
